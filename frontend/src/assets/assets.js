export const incident_list = [
    {
      id: 1,
      title: "Biased Recommendation Algorithm",
      description: "An AI-powered content recommendation engine consistently favored content aligned with specific political ideologies, causing an imbalance in the exposure of diverse perspectives. This bias was traced to training data skewed towards certain demographics and lacked proper oversight during model validation.",
      severity: "Medium",
      reported_at: "2025-03-15T10:00:00Z"
    },
    {
      id: 2,
      title: "LLM Hallucination in Critical Info",
      description: "A large language model deployed in a healthcare chatbot provided incorrect CPR procedure instructions during a critical emergency. The hallucinated information contradicted verified guidelines, potentially endangering human lives due to miscommunication and lack of real-time fact verification.",
      severity: "High",
      reported_at: "2025-04-01T14:30:00Z"
    },
    {
      id: 3,
      title: "Minor Data Leak via Chatbot",
      description: "A customer support chatbot inadvertently exposed internal user metadata including session IDs and timestamps when users asked certain generic queries. While the data was non-sensitive, the incident highlighted insufficient prompt sanitization and context leakage from backend logs.",
      severity: "Low",
      reported_at: "2025-03-20T09:15:00Z"
    },
    {
      id: 4,
      title: "Deepfake Voice Spoofing Attack",
      description: "An AI-generated deepfake voice clone of a company executive was used to trick employees into transferring funds to a fraudulent account. The attack bypassed voice-authentication systems due to lack of multi-factor verification and exploited publicly available voice samples.",
      severity: "High",
      reported_at: "2025-04-05T11:45:00Z"
    },
    {
      id: 5,
      title: "Autonomous Vehicle Pathing Error",
      description: "An autonomous vehicle misinterpreted a stop sign partially obscured by graffiti and failed to stop at a busy intersection. The systemâ€™s vision model lacked robustness against environmental noise, resulting in a near-miss incident involving pedestrian crossing.",
      severity: "High",
      reported_at: "2025-04-10T16:20:00Z"
    },
    {
      id: 6,
      title: "Toxic Content Generation by AI",
      description: "A generative AI used for creative writing assistance began producing violent and offensive text responses in response to benign prompts. This occurred due to a lapse in content filtering layers and inadequate adversarial testing during development.",
      severity: "Medium",
      reported_at: "2025-04-12T12:30:00Z"
    },
    {
      id: 7,
      title: "Facial Recognition Misidentification",
      description: "An AI-based facial recognition system used at a corporate security checkpoint falsely flagged a visiting executive as a known threat. This misidentification was due to racial bias in training data and a high false-positive rate for non-local faces.",
      severity: "Medium",
      reported_at: "2025-04-14T08:00:00Z"
    },
    {
      id: 8,
      title: "AI Model Reveals Sensitive Training Data",
      description: "A machine learning model trained on medical records was found to inadvertently reveal snippets of real patient data when prompted in certain ways. Lack of differential privacy techniques during training caused the leak.",
      severity: "High",
      reported_at: "2025-04-15T18:50:00Z"
    },
    {
      id: 9,
      title: "Inappropriate Ad Targeting by AI",
      description: "An AI-driven ad targeting system began showing gambling and alcohol-related advertisements to underage users due to poorly labeled age data and ambiguous user interest inference. This sparked regulatory concerns over AI compliance.",
      severity: "Medium",
      reported_at: "2025-04-17T09:25:00Z"
    },
    {
      id: 10,
      title: "Emotion AI Misjudged User Sentiment",
      description: "An emotion-detection AI misinterpreted a user's sarcasm as genuine aggression, triggering an unnecessary escalation protocol in a mental health support chatbot. The model lacked nuance in understanding tone and context.",
      severity: "Low",
      reported_at: "2025-04-19T07:40:00Z"
    }
  ]
  